---
layout: post
title: cs231n Lec.2 Softmax
categories: 学习记录cs231n
tags: Softmax_loss Code_by_Math
---
# cs231n Lec.2 Softmax

Assign1 中首先是让我们实现了合页损失函数，为了提升损失函数的可解释性,将原始损失函数转化为了softmax。回顾先前的合页损失：
$$
L_i = \sum_{j\neq y_i} max(0,s_{j}-s_{yi}+1)
$$
而softmax 的实现形式为：
$$
L_i = -log(\frac{e^{f_{yi}}}{\sum e^{fj}})
$$
其中使用到了对数似然的思想，loss为正确输出概率的负对数，当预测结果越准确时，所得损失越小。注意到内部将各分类器的输出作为$e$的指数，化为了正的数值，并且归一化保证$log$内的结果都在$0-1$之间。

在实现的过程中，需要注意避免指数操作时产生的溢出，以及数学公式到代码的实现。

第一点，溢出问题。由于原始的函数没有输出的范围，当输出的数值过大，而我们还是作为指数运算时，可能产生溢出，也就是不稳定。解决办法时在计算指数函数之前，将分类器的结果减去其最大值。
$$
f' = f - max(f)
$$
在实现过程中同样是分了两步走，首先是基于循环来实现的，其次通过向量化来加速运算。这个任务的难点一如既往是计算出Loss的具体表达。在计算之前需要先对符号进行约定：$i \in N , j \in C , k \in D$，其中N,C,D代表样本,类别，特征的数量。得到Loss的计算方法：
$$
\begin{aligned} Loss&= \frac{\sum_{i\in N }L_i}{N}\\
		& = \frac{\sum_{i\in N }-log(\frac{e^{f_{yi}}}{\sum e^{fj}})}{N}\\
		& = \frac{\sum_{i\in N }(-f_{yi}+log{\sum_{j \in C} e^{fj}})}{N}\\
		& \text{带入}f_j = v_j * x_j (v_j = W[:j]) :\\
		& = \frac{\sum_{i\in N }(-\sum_{k\in D }w_{k,y_i}x_{ik}+log{\sum_{j \in C} e^{\sum_{k\in D }w_{k,j}x_{ij}}})}{N}\\
\end{aligned}
$$
看着复杂，但是也就是得到分数后，直接计算第二个等式的内容计算即可，在求导时才用得到后续的等式。

下面计算导数形式，计算的导数都是对于$w_{ij}$而言的。
$$
\begin{aligned} \frac{\partial{Loss}}{\partial{w_{ij}}}
		& = \frac{\sum_{i\in N }\partial(-\sum_{k\in D }w_{k,y_i}x_{ik}+log{\sum_{j \in C} e^{\sum_{k\in D }w_{k,j}x_{ij}}})}{N\cdot \partial w_{ij}}\\
\end{aligned}
$$
由于允许使用循环，所以每一个样本（i）进行分类讨论：
$$
\frac{\partial{Loss}}{\partial{w_{ij}}} = \begin{cases}e^{f_{ij}}\cdot x_{ik},& j \neq y_i\\(-1+e^{f_{iy_i}})\cdot x_{ik} ,& j = y_i\end{cases}
$$
代码实现即可。