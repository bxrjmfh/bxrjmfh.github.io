---
layout: post
title: cs231n Lec.6 CNN Architectures
categories: 学习记录cs231n 深度学习 学习记录
tags: 
---
# cs231n Lec.6 CNN Architectures

## Batch Normalization 批归一化

批归一化的引入，是为了解决训练过程中偏置项大，权重之间相差较大的问题，需要把输入进行归一化处理。

![image-20221220154557071](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220154557071.png)

![image-20221220154940319](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220154940319.png)

标准化最简单的方法就是减去期望后除以标准差。是什么维度上的统计量呢？对于$N\times D$的输入$x$而言，均值和方差是就每一个通道来计算的。

![image-20221220155214105](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220155214105.png)

计算的时候是运行时动态计算均值和方差（待实现观察）：

![image-20221220161900436](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220161900436.png)

> Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift .
>
> 训练深度神经网络很复杂，因为每一层输入的分布在训练过程中都会发生变化，因为前一层的参数会发生变化。这通过要求较低的学习率和仔细的参数初始化来减慢训练速度，并且众所周知地难以训练具有饱和非线性的模型。我们将这种现象称为内部协变量偏移。

在机器学习模型中的一个重要假设是训练集与测试集的分布应当一致，这样训练出来的模型才是有效的。但是在多层的网络中由于参数的更新，这一点不能保证，即使是一个相同分布的输入，在网络更新的过程也会导致下层的结果不一致。

饱和非线性的含义需要和图像对比来看，对于一个激活函数而言,例如sigmoid：

![img](https://img-blog.csdnimg.cn/20190213101816654.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMzNzQxNTQ3,size_16,color_FFFFFF,t_70)

输入较大时，梯度处于较小的位置，更新速度变慢了，可以理解为饱和。

### BN的优点

其优点在于（test-time ）：

![image-20221220180835323](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220180835323.png)

### 归一化的几种不同形式

对于全连接和卷积网络而言归一化具有两种形式：

![image-20221220184151474](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220184151474.png)

卷积的归一化需要对$W,H$都进行操作，是因为卷积核会在图像的所有位置进行滑动。

实例（instance）归一化和层归一化，是对全连接和卷积的特殊操作，与BN的区别在于保留了样本间差异。如下所示：

![image-20221220200035557](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220200035557.png)

![image-20221220200109573](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220200109573.png)

![image-20221220200407572](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221220200407572.png)











