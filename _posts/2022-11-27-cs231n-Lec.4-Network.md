---
layout: post
title: cs231n Lec.4 Network
categories: 学习记录cs231n
tags: loss 反向传播 Code_by_Math
---
# cs231n Lec.4 Network

先前所学的是线性模型，输出都是原始数据的线性组合，为了解决非线性的问题，引入了神经网络的概念。神经网络的也可被称为是全连接网络，或者是多层感知机。

![image-20221126103528531](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221126103528531.png)

由于激活函数的存在，使得原始函数脱离了线性函数的范畴，每层的权重不再直接相关。常见的激活函数有：

![image-20221126104549879](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221126104549879.png)

在这里给出了网络的实现形式：

![image-20221126104940085](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221126104940085.png)

## 反向传播算法

> Notice that backpropagation is a beautifully local process. Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the *local* gradient of its output with respect to its inputs. Notice that the gates can do this completely independently without being aware of any of the details of the full circuit that they are embedded in. However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradient of its output value on the final output of the entire circuit. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs.

反向传播的过程不需要对整个网络的结构有所了解，只需要关注和其有关系的参数就可以完成梯度的计算。这也是求导过程（包括正向传播过程）可以并行计算加速的原因。为何被称作反向传播，我的理解是链式求导过程是由最终的输出推知梯度关于参数的导数。

### 一般函数

考虑代码实现部分（Assign. 1 layers.py）中的一个简单函数：

$$
O_{N\times C} = X_{N\times D } \cdot W_{D\times C} + b
$$

计算输出关于W的导数，由于每个$O_{ij}$的输出都是独立的，所以可以先分析一个变量的情况：

$$
\begin{aligned}O_{ij} &= \sum^{D}_{k} x_{ik}\cdot w_{kj}\\
					 \frac{\partial O_{ij}}{\partial w_{kj}} &= x_{ik}\\

\end{aligned}
$$

尝试将后面的结果中的下标$i$消除，得到关于所有输出的导数。下标$j$的处理有一点模糊，可以解释为$dW$和$O$之间的共同下标不用去除：

$$
\begin{aligned}
				dW_{kj}&=	 \sum_i^N \frac{\partial O_{ij}}{\partial w_{kj}} =  \sum_i^N x_{ik}\\

\end{aligned}
$$

在得到上式后，写开为矩阵形式：

$$
dW = \begin{bmatrix}\sum_i^N x_{i1},&\sum_i^N x_{i2},&\cdots &\sum_i^N x_{i1}\\
\sum_i^N x_{i1},&\sum_i^N x_{i2},&\cdots &\sum_i^N x_{i1}\\ &\cdots&\cdots \\ \sum_i^N x_{i1},&\sum_i^N x_{i2},&\cdots &\sum_i^N x_{i1}\end{bmatrix}_{D\times C}
$$

就得到了完全的的矩阵形式。从最基本的元素出发来推导，可以得到最终的表达。还有一部分是求取$db$，直接展示过程如下：

$$
\begin{aligned} \frac{\partial O_{ij}}{\partial b_j} &= 1\\
db_j &= \sum_i^N \frac{\partial O_{ij}}{\partial b_j} = N\\
\cdots & \cdots \\
db &= [N,N,\cdots,N]^T
\end{aligned}
$$

### Relu 函数（max）













