---
layout: post
title: 大媒体前沿：初探多模态
categories: 记录 论文阅读
tags: 记录 整理汇总
---
# 大媒体前沿：初探多模态

经过半个学期的课程学习，在未来媒体实验室的老师们分享了许多关于计算机视觉的前沿方向，这极大开阔了我的视野。为了对这一领域有更深入的了解，我查阅了许多资料，以求对多模态和跨域问题有一定认识。

## 关于多模态（multi-modal）

我们在生活中面对的常常就是多模态的信息。例如在上课时，老师讲授内容，这是声音信息，同时老师还会展示PPT，这是视觉信息。人可以轻易做到边听边看，可以回忆起来老师讲述的文字对应哪张课件，或者是按照PPT来复述老师所说的内容。以上这些就是一部分多模态的任务，包括映射（mapping）,语音合成等。

计算机是如何服务于多模态任务呢？就以映射任务为例，输入的是文本信息，输出的是一张图片，这本质上是对于不同数据信息的抽象。对于一段文字序列，可以通过embedding等方法来对一个词向量进行压缩表达，再使用LSTM等时序性质的网络结构进行特征提取。对于图像信息而言，卷积与全连接，最终也可以提取出特征来。

在得到特征后，需要对特征进行匹配。匹配方法主要有两种：基于示例的模型和生成式的模型。\cite{1}基于示例的模型财通字典机制进行匹配，而生成式的模型是采用字典训练一个模型来学习映射过程，类比于机器学习中的非参数方法和参数方法。

![image-20230108103057336](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20230108103057336.png)

对于前一种方法而言，可能会出现一对多的映射，这导致需要从众多的匹配项中选择一个最相关的出来。在特征空间中，可以通过距离来衡量各个向量的相似度，这样在一对多的映射中可以选择最相近的一个项来匹配。但这样处理的结果并不能产生较好的匹配度，因为空间中的相似性并不能够代表现实中的相似度。

在得到字典映射的结果后，如果是图生文的任务，还需要对得出的关键词进行排序和重新组合，这也带来了更多的后续任务。一种基于生成模型的办法，是由百度研究院在2014年所提出的m-RNN\cite{2}，这个模型就是完成图像解释的工作，在我之前的学习过程中，我只了解到LSTM等网络所构成的Seq-2-Seq模型，输入的信息是文字序列，而输出的序列是也是时序序列。

那么如何使用图像生成描述的句子呢？作者研究出了一个网络架构，如下所示：

![image-20230108110251747](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20230108110251747.png)

就和困难的小学生写作文一样，常常是打开作文本，看看周围的环境，然后边写边想，一个词一个词的写出一片完整的文章那样，由图像生成文字也是有这样的效果。句子生成是由概率来决定的，下一个词语是什么，是由前一个词所对应的条件概率所决定。这一条件概率是由网络的信息学习到的，正如图（b）中显示的网络结构所示，词向量经过两次特征编码来压缩表达，随后接入循环神经网络来标识前后文的信息，在后面multi-model的模块具有三个输入，分别是词编码，循环网络的输入，以及卷积网络的图像信息，三者结合起来形成了具有上下文特征的句子表达。

## 总结

在本门课程的学习中，我有幸接触到了计算机视觉方面的前沿知识。在了解前沿的过程中，我也对自己所学的CV和NLP的知识有了融合的感受，这也促使我对前沿的知识进行学习。在老师上课后，我会通过阅读论文，看博客文章来掌握网络的结构和思路，在日后还希望实现网络结构，让知识不仅仅停留在理论上。

@misc{baltrusaitis_multimodal_2017,
	title = {Multimodal {Machine} {Learning}: {A} {Survey} and {Taxonomy}},
	shorttitle = {Multimodal {Machine} {Learning}},
	url = {http://arxiv.org/abs/1705.09406},
	abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
	urldate = {2023-01-08},
	publisher = {arXiv},
	author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
	month = aug,
	year = {2017},
	note = {arXiv:1705.09406 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/lh/Zotero/storage/GHQVCNRA/Baltrušaitis 等 - 2017 - Multimodal Machine Learning A Survey and Taxonomy.pdf:application/pdf;arXiv.org Snapshot:/home/lh/Zotero/storage/HEFPZ6Q2/1705.html:text/html},
}

@misc{mao_deep_2015,
	title = {Deep {Captioning} with {Multimodal} {Recurrent} {Neural} {Networks} (m-{RNN})},
	url = {http://arxiv.org/abs/1412.6632},
	doi = {10.48550/arXiv.1412.6632},
	abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\textasciitilde}junhua.mao/m-RNN.html .},
	urldate = {2023-01-08},
	publisher = {arXiv},
	author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
	month = jun,
	year = {2015},
	note = {arXiv:1412.6632 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.10, I.2.6, I.2.7},
	file = {arXiv Fulltext PDF:/home/lh/Zotero/storage/NZ33JY6V/Mao 等 - 2015 - Deep Captioning with Multimodal Recurrent Neural N.pdf:application/pdf;arXiv.org Snapshot:/home/lh/Zotero/storage/23R7FRSM/1412.html:text/html},
}

