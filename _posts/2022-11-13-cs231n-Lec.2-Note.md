---
layout: post
title: cs231n Lec.2 Note
categories: 学习记录cs231n
tags: SVM_loss
---
# cs231n Lec.2 Note

课程链接:https://github.com/cs231n/cs231n.github.io/blob/master/classification.md



## 对 SVM loss 的理解

SVM loss 的形式：

$$\begin{aligned}
L_{i} &=\sum_{j \neq y_{i}}\left\{\begin{array}{ll}
0 & \text { if } s_{y_{i}} \geq s_{j}+1 \\
s_{j}-s_{y_{i}}+1 & \text { otherwise }
\end{array}\right.\\
&=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+1\right)
\end{aligned}$$

这里s是分类器的输出，含义为若原始数据对应的分类器标签$s_{y_i}$和其余分类器的输出$s_j$之间的距离大（$\geq 0$），那么就输出0（代表loss小），否则记作分类器输出结果的距离。距离选择1没有什么特殊的含义，仅仅表明二者之间是具有区别的。

![image-20221106102603971](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221106102603971.png)

（横轴：$s_{y_i}-s_j$）,当大于1的时候loss就变成0。

![image-20221106102813457](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221106102813457.png)

- Q1:仍然是0，没有发生变化
- Q2:min:0 max:$+\infin$
- Q3:c-1

![image-20221106103439761](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221106103439761.png)

- 变成原有的数值加1，但是考虑到loss的真实含义，我们认为在正确分类的情况下数值为0是更加合适的。

## 对Softmax的理解

在先前的工作中，分类器输出的是任意实数，不具有太大的解释性，而softmax方法将损失函数换为交叉熵损失来度量





