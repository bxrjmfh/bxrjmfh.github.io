---
layout: post
title: cs231n Lec3. Optimization
categories: 学习记录cs231n 
tags: 最优化方法 RMSprop Adam 自适应学习率
---
# cs231n Lec3. Optimization

[本节](https://cs231n.github.io/optimization-1/)是介绍优化相关的内容。

## 使loss下降的方法

1. 随机搜索

   在全局随机对所有的参数组合进行搜索，不会利用局部的信息。

2. 随机局部搜索

   这里利用了局部的信息，就当前的数据对当前点进行迭代，如果效果好就保留。

   $$
   W '  = W + \delta W_{radom}
   $$
   

3. 随梯度变化

   正如爬山过程中沿着下降最快的方向走能够最快下山那样，通过梯度来优化损失函数是一个好的想法，但是如何求取梯度呢？有两种方法：数值法与解析法，分别使用数值来估计以及微积分工具来解决。

   - 数值法

     基本思路是根据函数输入的大小，对输出的数值进行微小改变来计算其梯度。计算公式为：
   
     $$
     \nabla f(x) = \frac{f(x+\Delta h)+f(x-\Delta h)}{2\Delta h}
     $$
   
     只有当$\Delta h$ 的大小足够小的时候，得到的梯度才会相对精确，如图所示：

     ```python
     loss_original = CIFAR10_loss_fun(W) # the original loss
     print 'original loss: %f' % (loss_original, )
     
     # lets see the effect of multiple step sizes
     for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
       step_size = 10 ** step_size_log
       W_new = W - step_size * df # new position in the weight space
       loss_new = CIFAR10_loss_fun(W_new)
       print 'for step size %f new loss: %f' % (step_size, loss_new)
     
     # prints:
     # original loss: 2.200718
     # for step size 1.000000e-10 new loss: 2.200652
     # for step size 1.000000e-09 new loss: 2.200057
     # for step size 1.000000e-08 new loss: 2.194116
     # for step size 1.000000e-07 new loss: 2.135493
     # for step size 1.000000e-06 new loss: 1.647802
     # for step size 1.000000e-05 new loss: 2.844355
     # for step size 1.000000e-04 new loss: 25.558142
     # for step size 1.000000e-03 new loss: 254.086573
     # for step size 1.000000e-02 new loss: 2539.370888
     # for step size 1.000000e-01 new loss: 25392.214036
     ```

     此外，对于每一个维度都要计算一次梯度，对于现代的深度网络而言，其参数量过于巨大，可能不适宜于这类计算方法，因为计算的耗时太长。

   - 解析法

     为了解决效率上的问题，需要使用解析法来求取梯度，核心就是链式求导函数。但是解析法求取梯度也有容易出现错误的缺点。就比如痛苦的实现过程。

4. 梯度下降法（Gradient Descent）

   是根据现有的所有数据对损失函数进行计算，并且进行更新，这一步骤较慢。对数据集大的情况下不太实用。在梯度下降时批量的选取是一个超参数，通常在内存条件允许的情况下，选取2的幂次，以加速计算。

5. 小批量梯度下降

   对梯度下降的一个改进，不在全部的数据上计算梯度，而是在一个小批量进行计算。

6. 随机梯度下降（**Stochastic Gradient Descent (SGD)** , or also sometimes **on-line** gradient descent）

   是当批量为1的时候，小批量梯度下降的一个特殊情况。

## 其他的梯度下降方法

由于随机梯度下降法在局部极小点的时候可能停止迭代，所以要进行改动。

1. 使用动量

   ![image-20221125172835864](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125172835864.png)

   在函数优化的过程中，动量的解释由不改变原来偏移方向的能力变为了在原有梯度下继续运动的能力。这样在迭代过程中后续的动作还是会受到先前运动的影响。

2. 聂思捷洛夫动量（Nesterov Momentum）

   原始动量法和该方法的差异在于求解梯度的位置不同，聂氏动量的梯度在动量移动后的位置来计算，对于凸函数来说具有更强的理论收敛性。

   ![image-20221125174042131](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125174042131.png)

   在计算中一般是用$\tilde{x}$来代替原始的表达。

## 自适应的调整方法

1. AdaGrad 自适应梯度下降

   在梯度下降的基础上，统计了梯度的平方和，并且与原始的学习率相除，来保证学习率可以逐步下降，以收敛到良好的结果。

   ![image-20221125194306655](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125194306655.png)

   平方项是为了提升性能而设置的，后续还添加了平滑项，为了避免除以0的情况。这一方法由于使用了单调的学习率，可能在深度学习中带来过早停止的问题。

2. RMSProp

   ![image-20221125203323514](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125203323514.png)

   是对AdaGrad 的一个改进，将累计的误差计算方法做了修改。注意添加了衰减率（decay_rate），这样可以让衰减率不单调，这也是所谓Leaky的原因。衰减率的取值范围是[0.9, 0.99, 0.999]。

3. Adam

   ![image-20221125210025133](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125210025133.png)

   Adam损失函数有两项动量，学习率的迭代是两项相除的形式，取得了比RMSprop更好的效果。完整的Adam还补充了矫正机制，以修正初始动量为0的情况。

   ![image-20221125210955385](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125210955385.png)

## 对于学习率的调节

可以按照训练流程的推进，来把学习率进行调整。有余弦，直线，反平方根等调节方式。

![image-20221125212243759](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125212243759.png)

## 一些有趣的问题

对于最优化中所学的牛顿法等手段，似乎可以用于解决loss下降的问题，利用二阶泰勒公式来估计局部极值点的位置，可以接近极小点的位置。

![image-20221125212711730](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125212711730.png)

但是受限于参数规模太大，对于求逆矩阵等方法计算耗时过大，在n很大的时候已经不可行。

![image-20221125213314060](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125213314060.png)

使用拟牛顿法可以带来较好的效果，例如L-BFGS等方法。

![image-20221125213432358](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125213432358.png)

![image-20221125213501275](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125213501275.png)

![image-20221125213519172](https://lh-picbed.oss-cn-chengdu.aliyuncs.com/image-20221125213519172.png)
